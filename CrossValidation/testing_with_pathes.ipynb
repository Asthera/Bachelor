{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-28T09:13:37.558124Z",
     "start_time": "2024-02-28T09:13:37.480107Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cropped/video_frames/Sonoscape_2021-09_2021-10-18_002_20210930_154619_22.avi/frame_130.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 51\u001B[0m\n\u001B[1;32m     48\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;66;03m# Your training logic here\u001B[39;00m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/Global/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/Global/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/Global/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/Global/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[9], line 22\u001B[0m, in \u001B[0;36mOptimizedVideoFramesDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     20\u001B[0m frame_path \u001B[38;5;241m=\u001B[39m item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mframe_cropped_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Load image\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(frame_path)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Apply transformations if any\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n",
      "File \u001B[0;32m~/anaconda3/envs/Global/lib/python3.11/site-packages/PIL/Image.py:3247\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3244\u001B[0m     filename \u001B[38;5;241m=\u001B[39m fp\n\u001B[1;32m   3246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[0;32m-> 3247\u001B[0m     fp \u001B[38;5;241m=\u001B[39m builtins\u001B[38;5;241m.\u001B[39mopen(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3248\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   3250\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'cropped/video_frames/Sonoscape_2021-09_2021-10-18_002_20210930_154619_22.avi/frame_130.png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "class OptimizedVideoFramesDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.transform = transform\n",
    "        # Preprocess data: Flatten the list of frames\n",
    "        self.all_frames = [frame for video in data for frame in video['frames_only_label']]\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.all_frames[idx]\n",
    "        \n",
    "        frame_path = item['frame_cropped_path']\n",
    "        # Load image\n",
    "        image = Image.open(frame_path)\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Here, you can access video_info if needed, e.g., video_info['name_cvat']\n",
    "        # Example: Returning label, adjust as per your needs\n",
    "        label = item['label']  # Assuming label is at the video level\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Assuming `data` is your loaded JSON data\n",
    "data = json.load(open('fold_0.json'))\n",
    "\n",
    "\n",
    "# Initialize dataset with transformations\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images as needed\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = OptimizedVideoFramesDataset(data, transform=transform)\n",
    "\n",
    "# Initialize DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Example usage\n",
    "for images, labels in dataloader:\n",
    "    # Your training logic here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n",
      "77\n",
      "230\n",
      "{'name_cvat': '003_image_106405119193.mp4', 'label': 1, 'probe_type': 'lumify', 'subset': 'train', 'frame_count': 31, 'frame_count_only_label': 24, 'frames_only_label': [{'frame_id': '57TXFPHT', 'video_id': '84472RLO', 'name_cvat': '003_image_106405119193.mp4', 'frame': 140, 'frame_path': '/video_frames/003_image_106405119193.mp4/frame_140.png', 'frame_usg_area_idx_0': 96.0, 'frame_usg_area_idx_1': 650.0, 'frame_usg_area_idx_2': 478.0, 'frame_usg_area_idx_3': 796.0, 'frame_cropped_path': 'cropped/video_frames/003_image_106405119193.mp4/frame_140.png', 'aline': True, 'bline': False, 'liver': False, 'lung': False, 'lung_atelectasis': False, 'lungpoint': False, 'lungslidingabsent': False, 'lungslidingpresent': True, 'pleural_effusion': False, 's1': False, 's2': False, 'spleen': False, 'label': 1, 'size_h': 554.0, 'size_w': 318.0}, {'frame_id': 'OOZDKHLQ', 'video_id': '84472RLO', 'name_cvat': '003_image_106405119193.mp4', 'frame': 100, 'frame_path': '/video_frames/003_image_106405119193.mp4/frame_100.png', 'frame_usg_area_idx_0': 96.0, 'frame_usg_area_idx_1': 650.0, 'frame_usg_area_idx_2': 478.0, 'frame_usg_area_idx_3': 794.0, 'frame_cropped_path': 'cropped/video_frames/003_image_106405119193.mp4/frame_100.png', 'aline': True, 'bline': False, 'liver': False, 'lung': False, 'lung_atelectasis': False, 'lungpoint': False, 'lungslidingabsent': False, 'lungslidingpresent': True, 'pleural_effusion': False, 's1': False, 's2': False, 'spleen': False, 'label': 1, 'size_h': 554.0, 'size_w': 316.0}, {'frame_id': 'J8FSO1HJ', 'video_id': '84472RLO', 'name_cvat': '003_image_106405119193.mp4', 'frame': 130, 'frame_path': '/video_frames/003_image_106405119193.mp4/frame_130.png', 'frame_usg_area_idx_0': 96.0, 'frame_usg_area_idx_1': 650.0, 'frame_usg_area_idx_2': 478.0, 'frame_usg_area_idx_3': 796.0, 'frame_cropped_path': 'cropped/video_frames/003_image_106405119193.mp4/frame_130.png', 'aline': True, 'bline': False, 'liver': False, 'lung': False, 'lung_atelectasis': False, 'lungpoint': False, 'lungslidingabsent': False, 'lungslidingpresent': True, 'pleural_effusion': False, 's1': False, 's2': False, 'spleen': False, 'label': 1, 'size_h': 554.0, 'size_w': 318.0}, {'frame_id': '7VOXMGFN', 'video_id': '84472RLO', 'name_cvat': '003_image_106405119193.mp4', 'frame': 180, 'frame_path': '/video_frames/003_image_106405119193.mp4/frame_180.png', 'frame_usg_area_idx_0': 96.0, 'frame_usg_area_idx_1': 650.0, 'frame_usg_area_idx_2': 478.0, 'frame_usg_area_idx_3': 794.0, 'frame_cropped_path': 'cropped/video_frames/003_image_106405119193.mp4/frame_180.png', 'aline': True, 'bline': False, 'liver': False, 'lung': False, 'lung_atelectasis': False, 'lungpoint': False, 'lungslidingabsent': False, 'lungslidingpresent': True, 'pleural_effusion': False, 's1': False, 's2': False, 'spleen': False, 'label': 1, 'size_h': 554.0, 'size_w': 316.0}, {'frame_id': '1FIC0DZ8', 'video_id': '84472RLO', 'name_cvat': '003_image_106405119193.mp4', 'frame': 80, 'frame_path': '/video_frames/003_image_106405119193.mp4/frame_80.png', 'frame_usg_area_idx_0': 96.0, 'frame_usg_area_idx_1': 650.0, 'frame_usg_area_idx_2': 478.0, 'frame_usg_area_idx_3': 796.0, 'frame_cropped_path': 'cropped/video_frames/003_image_106405119193.mp4/frame_80.png', 'aline': True, 'bline': False, 'liver': False, 'lung': False, 'lung_atelectasis': False, 'lungpoint': False, 'lungslidingabsent': False, 'lungslidingpresent': True, 'pleural_effusion': False, 's1': False, 's2': False, 'spleen': False, 'label': 1, 'size_h': 554.0, 'size_w': 318.0}, {'frame_id': 'TCUKBP7M', 'video_id': '84472RLO', 'name_cvat': '003_image_106405119193.mp4', 'frame': 10, 'frame_path': '/video_frames/003_image_106405119193.mp4/frame_10.png', 'frame_usg_area_idx_0': 96.0, 'frame_usg_area_idx_1': 650.0, 'frame_usg_area_idx_2': 478.0, 'frame_usg_area_idx_3': 794.0, 'frame_cropped_path': 'cropped/video_frames/003_image_106405119193.mp4/frame_10.png', 'aline': True, 'bline': False, 'liver': False, 'lung': False, 'lung_atelectasis': False, 'lungpoint': False, 'lungslidingabsent': False, 'lungslidingpresent': True, 'pleural_effusion': False, 's1': False, 's2': False, 'spleen': False, 'label': 1, 'size_h': 554.0, 'size_w': 316.0}, {'frame_id': '23DKIM6F', 'video_id': '84472RLO', 'name_cvat': '003_image_106405119193.mp4', 'frame': 200, 'frame_path': '/video_frames/003_image_106405119193.mp4/frame_200.png', 'frame_usg_area_idx_0': 96.0, 'frame_usg_area_idx_1': 650.0, 'frame_usg_area_idx_2': 478.0, 'frame_usg_area_idx_3': 794.0, 'frame_cropped_path': 'cropped/video_frames/003_image_106405119193.mp4/frame_200.png', 'aline': True, 'bline': False, 'liver': False, 'lung': False, 'lung_atelectasis': False, 'lungpoint': False, 'lungslidingabsent': False, 'lungslidingpresent': True, 'pleural_effusion': False, 's1': False, 's2': False, 'spleen': False, 'label': 1, 'size_h': 554.0, 'size_w': 316.0}, {'frame_id': 'V1MQ3UDU', 'video_id': '84472RLO', 'name_cvat': '003_image_106405119193.mp4', 'frame': 120, 'frame_path': '/video_frames/003_image_106405119193.mp4/frame_120.png', 'frame_usg_area_idx_0': 96.0, 'frame_usg_area_idx_1': 650.0, 'frame_usg_area_idx_2': 478.0, 'frame_usg_area_idx_3': 794.0, 'frame_cropped_path': 'cropped/video_frames/003_image_106405119193.mp4/frame_120.png', 'aline': True, 'bline': False, 'liver': False, 'lung': False, 'lung_atelectasis': False, 'lungpoint': False, 'lungslidingabsent': False, 'lungslidingpresent': True, 'pleural_effusion': False, 's1': False, 's2': False, 'spleen': False, 'label': 1, 'size_h': 554.0, 'size_w': 316.0}, {'frame_id': 'Z94PXPPQ', 'video_id': '84472RLO', 'name_cvat': '003_image_106405119193.mp4', 'frame': 30, 'frame_path': '/video_frames/003_image_106405119193.mp4/frame_30.png', 'frame_usg_area_idx_0': 96.0, 'frame_usg_area_idx_1': 650.0, 'frame_usg_area_idx_2': 478.0, 'frame_usg_area_idx_3': 794.0, 'frame_cropped_path': 'cropped/video_frames/003_image_106405119193.mp4/frame_30.png', 'aline': True, 'bline': False, 'liver': False, 'lung': False, 'lung_atelectasis': False, 'lungpoint': False, 'lungslidingabsent': False, 'lungslidingpresent': True, 'pleural_effusion': False, 's1': False, 's2': False, 'spleen': False, 'label': 1, 'size_h': 554.0, 'size_w': 316.0}]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "file_path = \"metadata_folds/kkui-lung-bline-lumify:latest/video/fold_0.json\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_path = os.getcwd()\n",
    "# data is upper directory, so go one level up from current path\n",
    "data_path = os.path.join(current_path, os.pardir)\n",
    "normal_data_path = os.path.join(os.path.normpath(data_path), file_path)\n",
    "\n",
    "# Load the JSON file\n",
    "data = json.load(open(normal_data_path))\n",
    "\n",
    "# Get the frames from video the JSON file\n",
    "train_frames_json = [frame for video in data for frame in video['frames_only_label'] if video['subset'] == 'train']\n",
    "test_frames_json = [frame for video in data for frame in video['frames_only_label'] if video['subset'] == 'test']\n",
    "\n",
    "# Split the json frames into train, validation, and test sets\n",
    "train_frames_json, val_frames_json = train_test_split(train_frames_json, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the length of the train and test sets\n",
    "\n",
    "print(len(train_frames_json))\n",
    "print(len(val_frames_json))\n",
    "print(len(test_frames_json))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(data[0])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T10:43:40.893159Z",
     "start_time": "2024-02-28T10:43:37.233165Z"
    }
   },
   "id": "f43782295da0b5cc",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2b55ca4a9e3af399"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
